{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12\n",
    "\n",
    "Welcome to the final assignment for week 12 to 14.\n",
    "\n",
    "This assignment will be weighted 3 times a weekly one, as it is designed for 3 weeks. You will get a pass/fail for each part. Therefore, you are encouraged to also submit a partially solved notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Vision Transformer\n",
    "\n",
    "In Task 12 you implement the key concept of the transformer architecture by yourself and afterward train and evaluate a version of the Vision Transformer on CIFAR10. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12.1 Self-Attention\n",
    "\n",
    "Implement the attention mechanism by yourself. You are free to use torch and numpy to speed up the matrix multiplications, but please don't just use their transformer implementation.\n",
    "\n",
    "In the image below, you see the design of one Encoder Block. We want you to set up this Block. Please use your implementation of the Self-Attention (doesn't have to be multi-head) and build the Add & Norm and Feed Forward layers on top of it. Add & Norm and the Feed Forward should be implementations by PyTorch or else. You only need to use your own Self-Attention function.\n",
    "\n",
    "Show that your model block works, by forwarding a randomly initialized tensor through it once. Print the values of the Random input tensor, the output tensor and the Q,K and V matrices. **(RESULT)** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm\", height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 12.2 Compact Convolutional Transformer (CCT)\n",
    "\n",
    "This [Repository](https://github.com/SHI-Labs/Compact-Transformers) implements a few different approaches for Vision Transformers. We are interested in the CCT implementation. They provide a lot of information on how to use their code.\n",
    "\n",
    "* Clone their repository and train their smallest CCT model on the CIFAR10 dataset. A few epochs are enough. Report the classification accuracy and the number of epochs you trained. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 13: Self-Supervised Learning - Autoencoder\n",
    "\n",
    "For Task 13, your job is to train a model without the need of explicit labels in your dataset. One way to achieve this is by using an Encoder-Decoder Structure to learn representations of the data.\n",
    "\n",
    "Possible representations of our images are usually extracted after the final layer of our Encoder. So, for a given sample, a good representation is usually the output of the last Encoder layer, the light blue box in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:600/1*nqzWupxC60iAH2dYrFT78Q.png\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"https://miro.medium.com/v2/resize:fit:600/1*nqzWupxC60iAH2dYrFT78Q.png\", height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13.1 Baseline Convolutional Model\n",
    "\n",
    "To be able to evaluate against some baseline later, we want you to train a small ConvNet on the MNIST dataset and report the accuracy for this model.\n",
    "\n",
    "* Design a 5-7 layer deep ConvNet and train it on MNIST. Report its accuracy on the MNIST test split. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13.2 Autoencoder\n",
    "\n",
    "Now, we want you to implement a simple Encoder-Decoder Model using Convolutional Layers. You know various down- and upsampling techniques by now, make use of them. Also make sure to include Normalization Layers (e.g., BatchNorm, LayerNorm) and preferably ReLU-based activation functions.\n",
    "\n",
    "Autoencoders usually output the same dimensions as the input, to design a Reconstruction Loss. The goal is to compress the input's information using an Encoder model to a smaller representation that contains enough information to reconstruct the input well enough, using a Decoder model.\n",
    "\n",
    "* Build such an Autoencoder based on Conv Layers. Train it using an MSE-Loss as a reconstruction loss. **(RESULT)**\n",
    "* Then initialize a Feed-Forward model consisting of only 1 nn.Linear layer and **NO** non-linear activation function.The goal is to have this layer classify based on the representation input, i.e., output size of 10 (classes). **(RESULT)**\n",
    "* Get the representations for the full MNIST train split and use them as the input for training your small Feed-Forward model for 3-5 epochs. **(RESULT)**\n",
    "* Report the accuracy on the test split and compare to your baseline from 13.1. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 13.3 Masked Autoencoder (BONUS)\n",
    "\n",
    "Implement the Masked Autoencoder as close to the original as possible. [Paper](https://arxiv.org/pdf/2111.06377) Train it on MNIST and evaluate your representation quality equivalently to task 13.2. \n",
    "\n",
    "* Report the accuracy on the MNIST test split and compare to the baseline (13.1) and vanilla autoencoder (13.2). **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 14: Self-Supervised Learning - Contrastive Learning\n",
    "\n",
    "For Task 14, you should implement a Contrastive Learning approach for representation learning. Similarly to Task 13, we want you to set up a baseline model firstâ€”this time on CIFAR10 to have a colored image dataset.\n",
    "\n",
    "Contrastive Learning contrasts versions of the current sample against other samples within the batch. Therefore, we are working with augmentations. Augmentations of a specific image should still contain relevant features that are similar and ideally immune to these augmentations. This intuition is the core of our training objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14.1 Baseline Convolutional Model\n",
    "\n",
    "To be able to evaluate against some baseline later, we want you to train a small ConvNet on the CIFAR10 dataset and report the accuracy for this model.\n",
    "\n",
    "* Design a 5-7 layer deep ConvNet and train it on CIFAR10. Report its accuracy on the CIFAR10 test split. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 14.2 Contrastive Leaning \n",
    "\n",
    "Now implement a ConvNet-based model that has a Contrastive Loss objective. Refer to [SimCLR](https://arxiv.org/pdf/2002.05709v3), but any contrastive implementation is acceptable. We suggest implementing the loss using the dot product as a similarity measure instead of the distance between representations. The latter is less suitable in high-dimensional spaces.\n",
    "\n",
    "* Report the classification result using a one layer Feed-Forward model, equivalently to Task 13. Train the classifier model for 2-5 epochs at max. **(RESULT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratz, you made it! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
